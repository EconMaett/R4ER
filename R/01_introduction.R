# 01 - Introduction ---


## 1.1 What is the tidyverse? ----

# The tidyverse packages can be used to import,
# wrangle, program, and plot.
# Find out more on the official website: 
# https://www.tidyverse.org/


## 1.2 Importing -----

### 1.2.1 Reading from flat files ----

library(tidyverse)

# We can use the function `readr::read_csv()` to read a 
# comma-separated values (CSV) file from the web.

data_url <- "https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv"

covid_data <- read_csv(data_url)


# The tidyverse also provides packages for reading other data types.
# The `readxl` package is used to read Microsoft Excel spreadsheets,
# and the `haven` package to read data generated by proprietary software
# such as SPSS, Stata, and SAS.

# See more: 
# readxl: https://readxl.tidyverse.org/
# haven: https://haven.tidyverse.org/index.html


### 1.2.2 Reading from API ----

# An Application Programming Interface (API) is
# an interface to a database.

# REST APIs are common, with REST standing for
# representational State Transfer

# An application that adheres to the REST architectural constraints
# is colloquially described as RESTful.


# Create a request that specifies which data you want.

# Federal Reserve Bank of St. Louis Economic Data (FRED)
# API Documentation: 
# https://fred.stlouisfed.org/docs/api/fred/

# US Real Gross National Product: 
# https://api.stlouisfed.org/fred/series/observations?series_id=GNPCA&api_key=abcdefghijklmnopqrstuvwxyz123456&file_type=json

# The request can be broken down into the following parts:

#   1. The static URL to access the observations
#   https://api.stlouisfed.org/fred/series/observations?


#   2. The series ID that uniquely identifies US Real gross National Product data
#   id=GNPCA

#   3. Our personal API key
#   key=abcdefghijklmnopqrstuvwxyz123456


#   4. The file type of the output, in this case JSON
#   file_type=json

# All parameters after the static URL are separated by
# an ampersand (&) sign.

# The order of the parameters is not relevant.


# read monthly US Consumer Price Index (CPI) data,
# https://fred.stlouisfed.org/series/CPALTT01USM657N

# The series_id is "CPALTT01USM657N"

# If we only want data between January 2010 and December 2022,
# we add the parameters  observation_start and observation_end 
# in ISO Date format (YYYY-MM-DD).

# We use the `glue::glue()` function from the homonyms package
# to merge the parameters into a request using the ampersand (&)
# separators.

# See more at: https://glue.tidyverse.org/

# Note that to access data from FRED, you need to register
# for a personal key that you store as an environment
# variable called "api_fred_key" in the `.Renviron` file
# which you open with
# usethis::edit_r_environ()

# Afterwards you can access the FRED api key with
Sys.getenv("FRED_API_KEY")

# Use `glue::glue()` to create your HTTP GET request to the FRED API:
library(glue)

api_url       <- "https://api.stlouisfed.org/fred/series/observations?"
api_fred_key  <- Sys.getenv("FRED_API_KEY")
api_series_id <- "CPALTT01USM657N"
obs_start     <- "2010-01-01"
obs_end       <- "2022-12-01"
api_filetype  <- "json"
api_request   <- glue(
  "{api_url}series_id={
  api_series_id
  }&observation_start={
  obs_start
  }&observation_end={
  obs_end
  }&api_key={
  api_fred_key
  }&file_type={
  api_filetype
  }"
)

# Now we the function `httr::GET()` to connect to the FRED API,
# send the request and get the content with `httr::content()`

# See more at: https://httr.r-lib.org/index.html

# Note that `httr2` is now the recommended package.
# See more at: https://httr2.r-lib.org/

# Then we use `jsonlite::fromJSON()` to transform the JSON file to 
# a standard R object (a `base::list()`) and convert it 
# into a `tibble::tibble()`.

# See more at: https://jeroen.cran.dev/jsonlite/

# Notice that the CPI data is stored in the `base::list()`
# element called "observations".
# This is specific to this API.
library(tidyverse)
library(httr)
library(jsonlite)

cpi_request <- GET(url = api_request)
cpi_content <- content(cpi_request, as = "text")
cpi_list    <- fromJSON(cpi_content, flatten = FALSE)
cpi_tbl     <- cpi_list[["observations"]] |> as_tibble()

print(cpi_tbl)
# realtime_start, realtime_end, date, value

# The official tidyverse website provides a comprehensive list
# of all supported file formats and the respective
# packages to handle them
# See here: https://www.tidyverse.org/packages/#import

# The `rio` package also enables import from multiple file types.
# See here: http://gesistsa.github.io/rio/


## 1.3 Wrangling ----

# We use the functions from the `dplyr` package to
# get the data ready to use.
# See more: https://dplyr.tidyverse.org/

### 1.3.1 Data manipulation ----

# We use the COVID data set imported from Our World in Data (OWID).

# The `dplyr::glimpse()` function lets us have a grasp of the data.
covid_data |> 
  glimpse()

# Let's say we are only interested in the new COVID cases
# (column new_cases) in Brazil (column location).

# The tidyverse assigned names (verbs) to functions
# according to the actions they perform - many are
# admittedly SQL-inspired.

# `dplyr::distinct()` drops all observations (rows) that are not unique,
# whereas select() picks variables based on their names or positions.

# The `dplyr::filter()` function retains the rows that satisfy
# a given condition, the analogue of the SQL WHERE.

# Such logical conditions either return TRUE or FALSE.

covid_data_sub1 <- covid_data |> 
  distinct() |> 
  select(date, continent, location, new_cases) |> 
  filter(location == "Brazil")

print(covid_data_sub1)

# We may guess that cases are under reported and 
# want to create a new column that shows twice the
# number of recorded new cases.

# We also create a column that records the dominant
# strain at each time period.

# We know that the Delta strain took over by the end of July 2021
# and that Omicron overtook Delta beginning 2022.


# The `dplyr::mutate()` verb is used to create new variables from existing ones.

# `dplyr::case_when()` is an SQL-inspired function that creates conditions
# inside the `dplyr::mutate()` statement.

# It returns NA if no condition is met.

# The workaround is to define an extra condition as
# TRUE ~ value

covid_data_sub2 <- covid_data_sub1 |> 
  mutate(
    real_new_cases = 2 * new_cases,
    dominant_stran = case_when(
      date <= "2021-07-31" ~ "Gamma",
      date > "2021-07-31" & date <= "2021-12-31" ~ "Delta",
      date > "2021-12-31" & date <= "2022-02-01" ~ "Omicron",
      TRUE ~ "We don't know"
    )
  )

print(covid_data_sub2)

# The `dplyr::between()` function is a shortcut for numeric conditions
# that are bounded both on the left and the right.

# It also works with dates if we declare the arguments as
# date objects.

# We can replace conditions 2 and 3 in order to have more compact
# and efficient code (it is implemented in C++, like many
# modern R functions).

covid_data_sub2 <- covid_data_sub1 |> 
  mutate(
    real_new_cases = 2 * new_cases,
    dominant_strain = case_when(
      date <= "2021-07-31" ~ "Gamma",
      between(date, as.Date("2021-07-31"), as.Date("2021-12-31")) ~ "Delta",
      between(date, as.Date("2021-12-31"), as.Date("2022-02-01")) ~ "Omicron",
      TRUE ~ "We don't know"
    )
  )


print(covid_data_sub2)
# We might be interested in new cases in all European countries.

# We use the `dplyr::group_by()` function in conjunction with
# `dplyr::mutate()` or `dplyr::summarise()`.


# Suppose we want to know which European country recorded
# the highest number of new Covid cases in a single day
# by mid 2022.

# Don't forget to `dplyr::ungroup()` as soon as you don't need
# to perform grouped operations anymore.

covid_data_sub3 <- covid_data |> 
  distinct() |> 
  filter(
    continent == "Europe",
    date <= "2022-06-30",
    !is.na(new_cases)
  ) |> 
  group_by(location) |> 
  summarise(max_new_cases = max(new_cases)) |> 
  ungroup() |> 
  arrange(desc(max_new_cases))

print(covid_data_sub3)

# Some countries such as Spain, Portugal and France
# will return NA if we do not include the condition
# `!is.na(new_cases)` inside of `dplyr::filter()`.

# If we use `base::max(new_cases, na.rm = TRUE)`,
# countries with no data in new_cases will return 
# -Inf.

# Filtering first solves this problem.


# We want to find the date at which the highest number
# of new cases occurred (the peak date).

covid_data_sub5 <- covid_data |> 
  distinct() |> 
  filter(
    continent == "Europe",
    date <= "2022-06-30",
    !is.na(new_cases)
  ) |> 
  group_by(location) |> 
  summarise(
    max_new_cases = max(new_cases),
    peak_date = date[which(new_cases == max_new_cases)] |> max()
  ) |> 
  ungroup() |> 
  arrange(desc(max_new_cases), peak_date) |> 
  slice(1:10)

print(covid_data_sub5)


# `dplyr::mutate()` and `dplyr::summarise()` allow you to use a
# variable that you have just created in a subsequent
# task inside the same calling.


# Merging multiple data sets is achieved by the 
  # `*_join()` and `*_bind()` family of functions from the `dplyr` package.


# If we want to add the column Pop_Size from 
# europe_population to covid_data_sub5 matching
# rows based on the location column.
europe_population <- covid_data |> 
  distinct() |> 
  filter(continent == "Europe") |> 
  select(location, population) |> 
  group_by(location) |> 
  summarise(Pop_size = mean(population)) |> 
  rename(Country = location) |> 
  arrange(desc(Pop_size))

print(europe_population)

# the by argument is needed since the key columns
# have different names in the two datasets.
covid_data_sub5_with_pop <- covid_data_sub5 |> 
  left_join(europe_population, by = c("location" = "Country"))

print(covid_data_sub5_with_pop)


# We can add the information on the maximum daily number
# new cases and peak dates from covid_data_sub5
# to european_population with right_join().

pop_with_covid_info <- covid_data_sub5 |> 
  right_join(
    europe_population,
    by = c("location" = "Country")
  )

print(pop_with_covid_info)

# Use `dplyr::full_join()` to keep all the data and produce NAs
# and `dplyr::inner_join()` to receive only keys that exist in both
# data sets.


# left_join(), right_join(), full_join() and inner_join()
# are the four mutating-joins.

# filtering-joins filter the rows from one data set
# based on the presence (semi_join()) or absence 
# (anti_join()) of matches in the other data set.


# Suppose the Covid data set were released in single
# files, one for each country

covid_fr <- covid_data |> 
  distinct() |> 
  select(date, location, new_cases, total_cases) |> 
  filter(location == "France") |> 
  arrange(date)

print(covid_fr)

covid_uk <- covid_data |> 
  distinct() |> 
  select(date, location, new_cases) |> 
  filter(location == "United Kingdom") |> 
  arrange(date)

print(covid_uk)


# We use `dplyr::bind_rows()` to combine several data sets.

# Non-matching columns will be kept with their values
# filled with NA in case a column is absent.

covid_fr_uk <- bind_rows(covid_fr, covid_uk)

print(covid_fr_uk)


# The bind_cols() function will combine two data sets
# by matching row position rather than key variable.

# This means the two data sets need to have the same length.


### 1.3.2 Data layout ----

# The `tidyr` package helps us create tidy data.
# See more: https://tidyr.tidyverse.org/

# Data is tidy when every column is a variable and
# every row is an observation
# and each cell is a single value.


# Tidy data is known as the wide format, as opposed
# to the long format.

# Ask yourself the following question:
# Is all the information contained in a single cell?
# If yes, the data is in wide or tidy format.
# If no, it is in long format.


# Use `tidyr::pivot_wider()` to convert from long to wide format.

# names_from is the column we want to widen,
# values_from is the column containing the observations
# that will fill each cell.
# "id_cols" is a parameter used to declare the set of 
# columns that uniquely identifies each observation.
# All other columns will be dropped.

covid_wide_01 <- covid_data |> 
  pivot_wider(
    id_cols = "date",
    names_from = "location",
    values_from = "new_cases"
  )

print(covid_wide_01)

# If we want to have both new_cases and new_deaths
# for all countries, we provide a vector with the
# desired variables in "values_from".

# New columns will be named according to the pattern
# "variable_location".


# Because our variables are already separated
# by a single underscore, we can demand that our
# new variables be separated by a double underscore.

covid_wide_02 <- covid_data |> 
  pivot_wider(
    id_cols = "date",
    names_from = "location",
    values_from = c("new_cases", "new_deaths"),
    names_sep = "__"
  )

print(covid_wide_02)


# The long format is preferable over the wide format
# when the data set contains more than one grouping
# variable or we may want to work on more than one variable.

# Long format data sets are also ideal for plotting with
# the `ggplot2` package.


# Use `tidyr::pivot_longer()` to convert data from wide to long format
# and specify the cols argument to declare what
# columns you want to stack.

covid_long_01 <- covid_wide_02 |> 
  pivot_longer(
    cols = -"date",
    names_to = c("variable", "location"),
    values_to = "value",
    names_sep = "__"
  )

print(covid_long_01)

# We have specified that we want to leave out the
# date column with the minus sign (-).


# Suppose the location column included both the
# continent and the country names.
covid_loc_cont <- covid_data |> 
  distinct() |> 
  select(location, continent, date, new_cases, new_deaths) |> 
  mutate(location = paste0(location, "_", continent)) |> 
  select(-continent)

print(covid_loc_cont)

# We use the `tidyr::separate()` function to solve this.
covid_separate <- covid_loc_cont |> 
  separate(
    col = "location",
    into = c("location", "continent"),
    sep = "_"
  )

print(covid_separate)


# This is more difficult if the separators are non-trivial.
covid_loc_cont <- covid_data |> 
  distinct() |> 
  select(location, continent, date, new_cases, new_deaths) |> 
  mutate(location = paste0(location, continent)) |> 
  select(-continent)

print(covid_loc_cont)

# Ideally we would provide e regular expression (regex)
# to match the appropriate pattern to split the string
# into location and continent.


# Use the `tidyr::unite()` function to convert covid_separate
# back to "covid_loc_cont".
covid_unite <- covid_separate |> 
  unite(
    col = "location",
    c("location", "continent"),
    sep = "_"
  )

print(covid_unite)


# The `tidyr::nest()` function in conjunction with `dplyr::group_by()`
# is used to create a new data format in which
# every cell is a `base::list()` object rather than a single
# observation.

covid_eu_nest <- covid_data |> 
  filter(continent == "Europe", !is.na(total_cases)) |> 
  group_by(location) |> 
  nest()

print(covid_eu_nest)

# Nested data is useful when you need to perform
# several tasks over whole data sets, for example
# model fits for each country or plots for each country.

# The purrr package which enables functional programming
# is indispensable in this context.


### 1.3.3 Text manipulation ----

# The `stringr` package is used for text manipulation
# in the tidyverse.
# See more: https://stringr.tidyverse.org/index.html

# It is based on the underlying stringi package.
# https://stringi.gagolewski.com/


# We use `stringr::str_detect()` in conjunction with dplyr::filter()
# since it returns TRUE if the specific pattern is
# found in the string and FALSE otherwise.


# Assuming we want to analyze the Covid data only for
# the North and South Americas.
covid_americas <- covid_data |> 
  filter(
    continent %in% c("North America", "South America")
  )

print(covid_americas)


# Let's pretend we had 50 continents on Earth
# with 25 having "America" in their names.
# `stringr::str_detect()` will help us here.
covid_americas <- covid_data |> 
  filter(
    stringr::str_detect(continent, "America")
  )

print(covid_americas)


# We can provide multiple patterns to `stringr::str_detect()`
# by separating them with `|`.
covid_americas <- covid_data |> 
  filter(
    stringr::str_detect(continent, "South|North")
  )

print(covid_americas)


# We use `negate = TRUE` if we only want the strings
# that don't match our pattern.
covid_exAsia <- covid_data |> 
  filter(
    str_detect(continent, "Asia", negate = TRUE)
  )

print(covid_exAsia)

# Suppose we have a data set "covid_numCont"
# in which the observations in the continent
# column starts with a random number from 0 to 9
# due to a typing error.
covid_numCont <- covid_data |> 
  distinct() |> 
  select(continent, location, date, new_cases) |> 
  mutate(
    continent = paste0(floor(runif(n = nrow(covid_data), min = 0, max = 9)), ".", continent)
  )

print(covid_numCont)


# We want to subset the strings in continent from position
# three onward using `strignr::str_sub()`.

# The end argument defaults to the last character so
# we don't need to make it explicit.

covid_numCont |> 
  mutate(
    continent = str_sub(continent, start = 3)
  )


# But what if the random numbers had ranged from
# 0 to 10?
covid_numCont <- covid_data |> 
  distinct() |> 
  select(continent, location, date, new_cases) |> 
  mutate(
    continent = paste0(ceiling(runif(n = nrow(covid_data), min = 1, max = 10)), ".", continent)
  )

print(covid_numCont)

# We use a regular expression inside `stringr::str_remove()`
# to get rid of everything before and up to the
# dot (`.`), with `".*\\."`.
covid_numCont |> 
  mutate(
    continent = str_remove(continent, ".*\\.")
  )


# The data set "covid_type" has two different types
# for North America.
# A missing "h" in rows 1 and 5 and
# an extra "h" in rows 3 and 8

covid_typo <- covid_data |> 
  distinct() |> 
  select(continent, location, date, new_cases) |> 
  filter(continent == "North America")

covid_typo[1, ]$continent <- "Nort America"
covid_typo[5, ]$continent <- "Nort America"

covid_typo[3, ]$continent <- "Northh America"
covid_typo[8, ]$continent <- "Northh America"

print(covid_typo)


# We use `stringr::str_replace()` to fix the whole column.
covid_typo |> 
  mutate(
    continent = str_replace(
      continent,
      "Nort America",
      "North America"
    ),
    continent = str_replace(
      continent,
      "Northh America",
      "North America"
    )
  )

# Or we pass a vector with all replacements to the
# `stringr::str_replace_all()` function.
covid_typo |> 
  mutate(
    continent = str_replace_all(
      continent,
      c(
        "Nort America" = "North America",
        "Northh America" = "North America"
      )
    )
  )


# A common typo is whitespace.
# The `stringr` package provides the functions
# `str_trim()` and str_squish().

# `stringr::str_trim()` removes whitespaces from the start/end
# of the string.

# `stringr::str_squish()` removes whitespaces from inside a string.


# The data set "covid_ws" has a whitespace in the end
# of observations 1 and 5 and a repeated whitespace
# inside the observations 3 and 8

covid_ws <- covid_data |> 
  distinct() |> 
  select(continent, location, date, new_cases) |> 
  filter(continent == "North America")

covid_ws[1, ]$continent <- "Nort America "
covid_ws[5, ]$continent <- "Nort America "

covid_ws[3, ]$continent <- "Northh  America"
covid_ws[8, ]$continent <- "Northh  America"

print(covid_ws)

# Note that `tibble::tibble()` objects automatically add
# quotation marks around the strings to highlight
# the extra white spaces!


# Use `stringr::str_trim()` and `stringr::str_squish()` sequentially
# to get rid of the extra whitespace.
covid_ws |> 
  mutate(
    continent = continent |> 
      str_trim() |> 
      str_squish()
  )


# Employ `stringr::str_extract()` to extract the name of the continents
# in conjunction with `stringr::str_remove()` to remove them from the
# original column.
covid_loc_cont2 <- covid_data |> 
  distinct() |> 
  select(location, continent, date, new_cases, new_deaths) |> 
  mutate(location = paste0(location, continent)) |> 
  select(-continent)

print(covid_loc_cont2)


# Multiple patterns can be separated by `|`.
continents <- c(
  "Asia",
  "Europe",
  "Africa",
  "South America",
  "North America",
  "Oceania"
  ) |> 
  paste0(collapse = "|")

print(continents)
# "Asia|Europe|Africa|South America|North America|Oceania"

covid_loc_cont2 |> 
  mutate(
    continent = str_extract(location, continents),
    location = str_remove(location, continents)
  )

# This is a creative solution!


### 1.3.4 Date manipulation ----

# The `lubridate` package supports handling date objects.
# See more: https://lubridate.tidyverse.org/

# The standard ISO 8601 date format YYYY-MM-DD is achieved
# with the  `lubridate::ymd()` function.
# See more: https://www.iso.org/iso-8601-date-and-time-format.html

# It works regardless of how the terms are separated,
# be it 2022/12/01 or 20221201 or 2022-12-01.

# Both full and abbreviated month names are allowed
# 2021-December-01 or 2021-Dec-01

# The same logic applies to the related functions
# mdy(), dmy(), ymd(), dym(), my(), ym() and so on.

library(lubridate)
ymd("2022/12/01")

mdy("december, 1, 2022")

dmy("01122022")

my("Dec-2021")


# If the string format does not match any of the predefined
# patterns, use lubridate::as_date() to declare
# the unusual format, specifying %Y for year,
# %m for month, and %d for day.

# %b represents month names (full or abbreviated)
# and %y represents two-digit years.

# Use the `priecR` package 
# See more: https://stevecondylios.github.io/priceR/


# Got to the website https://exchangerate.host/
# and sign up for a free account, and call
# usethis::edit_r_environ()
# to save your API key as "EXCHANGERATEHOST_ACCESS_KEY".

# Restart R
library(tidyverse)
library(lubridate)
library(priceR)

# Check that R recognizes your API key:
Sys.getenv("EXCHANGERATEHOST_ACCESS_KEY")


# Get historical Brazilian Real (BRL) vs US Dollar
# (USD) exchange rates:
brl_usd <- historical_exchange_rates(
  from = "USD", 
  to = "BRL",
  start_date = "2010-01-01",
  end_date = today()
  )

colnames(brl_usd) <- c("date", "brl")

brl_usd <- brl_usd |> 
  as_tibble() |> 
  mutate(date = str_replace_all(date, "-", "/"))

print(brl_usd)


# We fist convert the column date with `lubridate::dmy()`
brl_usd_aux <- brl_usd |> 
  mutate(date = ymd(date))

print(brl_usd_aux)


# Suppose we want to obtain BRL monthly averages.

brl_usd_monthly <- brl_usd_aux |> 
  mutate(
    year = year(date),
    month = month(date)
  ) |> 
  group_by(year, month) |> 
  summarise(brl_monthly = mean(brl))

print(brl_usd_monthly)

# We use `lubridate::make_date()` to recover the date format
# "YYYY-MM-DD"

# For the day parameter, we either set it to 1 (default)
# or to the last day of the month using 
# `lubridate::days_in_month()`:

brl_usd_monthly |> 
  mutate(
    date = make_date(
      year = year,
      month = month,
      day = 1
    ),
    date2 = make_date(
      year = year,
      month = month,
      day = days_in_month(date)
    )
  )


# Note that creating a column with the number of days in
# each month is itself often useful.

# If we have only the month totals of a variable and we
# need to compute daily averages for example.


# Next we want quarterly means.
# Be aware that the `lubridate::quarter()` function has a parameter
# named `with_year` that when set to TRUE eliminates
# the need to create a separate column for the year.

brl_usd_quarterly <- brl_usd_aux |> 
  mutate(
    quarter = quarter(date, with_year = TRUE)
  ) |> 
  group_by(quarter) |> 
  summarise(brl_quarterly = mean(brl))

print(brl_usd_quarterly)


# Special attention is needed when working with
# weeks, because the `lubridate` package provides
# the functions `week()` and `isoweek()`.
week("2022-01-01")
# 1

# `lubridate::week()` returns the number of complete 
# seven day periods that occurred between the date 
# and January 1st.

# `lubridate::isoweek()` returns the number of the week
# (from Monday to Sunday) the date belongs to.
isoweek("2022-01-01")
# 52


# You may need to change the language settings
Sys.setlocale("LC_TIME", "English")

weekdays(date("2022-01-01"))
# "Saturday"

# Because this day is a Saturday, `lubridate::isoweek()` 
# will include it in the last week of the previous year.


# To compute weekly averages, meaning averages of 
# seven day periods, we use `lubridate::isoweek()`.


# We can also extract the week day from date objects.
brl_usd_aux |> 
  mutate(wday = wday(date, label = TRUE))


# The `lubridate` package imports the special operators
# `%m+%` and `%m-%` that allow addition and subtraction 
# of date objects.
d1 <- ymd("2020-02-29")
d1 %m+% years(2)
# "2022-02-28"

d1 %m-% months(3)
# "2019-11-29"

d1 %m+% days(1)
# "2020-03-01"


# The lesser known `lubridate::add_with_rollback()` function 
# gives us more control over the output.

# When adding one month to 2022-01-31 we might want
# either 2022-02-28 (the last day of the next month)
# or 2022-03-01 (a period of one month).


# To get the latter, we set the `roll_to_first` parameter
# to TRUE.
d2 <- ymd("2022-01-31")
add_with_rollback(e1 = d2, e2 = months(1), roll_to_first = TRUE)
# "2022-03-01"

add_with_rollback(e1 = d2, e2 = months(1), roll_to_first = FALSE)
# "2022-02-28"


# The functions `floor_date()` and `ceiling_date()`
# will round date objects down or up, respectively,
# to the nearest boundary of the specified time unit.
d3 <- ymd("2021-03-13")

floor_date(d3, unit = "month")
# "2021-03-01"

ceiling_date(d3, unit = "month")
# "2021-04-01"

floor_date(d3, unit = "quarter")
# "2021-01-01"

ceiling_date(d3, unit = "quarter")
# "2021-04-01"


# These functions are helpful if we want to match or re-write
# dates that refer to the same period but are written differently,
# (monthly date in set A is "2021-12-01" and in set B is "2021-12-31").

# They can also be used to perform temporal aggregation.
# Instead of creating two new grouping columns called "year"
# and "month", as in the previous example, we can accomplish
# the same result by rounding dates down, allowing us to
# preserve the "date" function.
brl_usd_monthly2 <- brl_usd_aux |> 
  mutate(
    date = floor_date(date, unit = "month")
  ) |> 
  group_by(date) |> 
  summarise(brl_monthly = mean(brl))

print(brl_usd_monthly2)


# To finish up, we have a quick look at the `lubridate` family
# of functions `wday()`, `mday()`, `qday()` and `yday()`.

# They can be used to get the number of days that have 
# occurred within a time period.
wday("2021-06-10") # 5th day of that week
# 5 

mday("2021-06-10") # 10th day of that month (obviously)
# 10

qday("2021-06-10") # 71th day of the second quarter of 2021
# 71

yday("2021-06-10") # 161the day of 2021
# 161

# These functions are very useful when you need to compare
# observations from the same period in different years
# or create **high frequency seasonal variables**.


## 1.4 Looping ----

# Iteration is an indispensable tool in every programming language.

# Loops allow us to repeat an action over a set of values,
# thus preventing us from copying-and-pasting things over and over.

# The `purrr` package provides tools to work with functions
# and vectors.
# See more: https://purrr.tidyverse.org/

# For now, we stick to the `purrr::map_*()` family of functions.

# These always follow the same logic:
# Apply a function - existing or user-defined
# - over a `base::vector()` or `base::list()` of arguments.


# Suppose we have three numeric vectors
v1 <- c(1, 4, 7, 8)
v2 <- c(3, 5, 9, 0)
v3 <- c(12, 0, 7, 1)
# and we want to compute the `base::mean()` for each vector.

# We simply put the three vectors into a `base::list()` object
v_all <- list(v1, v2, v3)

# and use `purrr::map()`
map(.x = v_all, .f = mean)

# Note that by default, the return object is of type `base::list()`.

# Other types of return object are available in functions called
# `purrr::map_*()`;
# `map_dbl()` returns a numeric vector,
# `map_dfc()` returns a column data frame.

map_dbl(.x = v_all, .f = mean)
# 5.00 4.25 5.00

map_dfc(.x = v_all, .f = mean)
# This creates column names ...1, ...2, ...3.

# Usually, `base::list()` return objects are preferable,
# since we can easily use them for further operations.

# A data frame is the best choice for final results.


# Next, we provide a custom function.
# Additionally to the CPI, we also want GDP and the Unemployment
# Rate from the FRED API.

# Previously we created the object
print(api_series_id)
# that contained the ID of the CPI time series 

# We create a function whose only parameter is the
# FRED series ID.

# Then we create a vector or list of the desired IDs
# and use them inside the `purrr::map()` function.


# We call the function `get_series()` in classical API notation.
# We leave `api_series_id` as the parameter `series_id`.

# We keep some objects with their original names, like `cpi_`
# to avoid confusion.
library(glue)

get_series      <- function(series_id) {
  api_url       <- "https://api.stlouisfed.org/fred/series/observations?"
  api_key       <- api_fred_key
  api_series_id <- series_id
  obs_start     <- "2010-01-01"
  api_filetype  <- "json"
  api_request   <- glue::glue(
    "
    {api_url}series_id={
    api_series_id
    }&observation_start={
    obs_start
    }&api_key={
    api_key
    }&file_type={
    api_filetype
    }
    "
  )
  cpi_request <- httr::GET(url = api_request)
  cpi_content <- httr::content(cpi_request, as = "text")
  cpi_list    <- jsonlite::fromJSON(txt = cpi_content, flatten = FALSE)
  cpi_tbl     <- cpi_list[["observations"]] |> tibble::as_tibble()
  return(cpi_tbl)
}

# We test the function with the CPI identifier used previously:
get_series(series_id = "CPALTT01USM657N")
# A tibble: 168 x 4
# realtime_start realtime_end date value

# This works fine. Next we create a vector or list witht eh IDs for
# CPI, GDP, and unemployment rate.

# Assigning a name to each element in a vector or list is always
# a good idea!
id_list <- list(
  "CPI" = "CPALTT01USM657N",
  "GDP" = "GDPC1",
  "Unemp" = "UNRATE"
)

print(id_list)


# Use `id_list` as the argument ".x" inside of `purrr::map()`
# and apply the custom function `get_series()` to each element
# in the list.
# Name the resulting `tibble()` object "fred_data".
fred_data <- purrr::map(
  .x = id_list,
  .f = get_series
)

print(fred_data)


# We can make the function more general by allowing more 
# parameters to vary.

# If we wanted to have different time spans for each series,
# we would simply use `purrr::map2(.x, .y, .f)` which
# allows us to use two input arguments for a function.
library(glue)

get_series2par <- function(series_id, series_start) {
  api_url       <- "https://api.stlouisfed.org/fred/series/observations?"
  api_key       <- api_fred_key
  api_series_id <- series_id
  obs_start     <- series_start
  api_filetype  <- "json"
  api_request   <- glue::glue(
    "
    {api_url}series_id={
    api_series_id
    }&observation_start={
    obs_start
    }&api_key={
    api_key
    }&file_type={
    api_filetype
    }
    "
  )
  cpi_request <- httr::GET(url = api_request)
  cpi_content <- httr::content(cpi_request, as = "text")
  cpi_list    <- jsonlite::fromJSON(txt = cpi_content, flatten = FALSE)
  cpi_tbl     <- cpi_list[["observations"]] |> tibble::as_tibble()
}

# Create the parameter list "time_list"
time_list <- list(
  "CPI" = "2010-01-01",
  "GDP" = "2012-04-01",
  "Unemp" = "2014-06-01"
)

fred_data2 <- purrr::map2(
  .x = id_list,
  .y = time_list,
  .f = get_series2par
)
print(fred_data2)


# Note that `purrr::map2()` does not take into account the
# variable names of its inputs. The sequence of the inputs
# therefore matters!

# A safer alternative would be to use the names instead of
# the positions as indexes.

# Since we can use `list[["element_name]]` to access
# an element of a list by its name, we can reduce
# our problem to a single-dimension one by looping over
# the variable names (which are the same in both lists).

vars_fred <- c("CPI", "GDP", "Unemp")

fred_data2_names <- purrr::map(
  .x = vars_fred,
  .f = function(x) get_series2par(
    series_id = id_list[[x]],
    series_start = time_list[[x]]
    )
  ) |> 
  magrittr::set_names(vars_fred)


print(fred_data2_names)


# Notice how powerful this solution is:
# you can generalize it to as many parameters as you need
# without taking the risk of using the parameter value of
# one series into another, with the additional work being
# only to make explicit the parameters of the function in `.f`.


# To finish this topic, we go back to the end of the subsection
# on data layout. There, we had an overview of nested data frames
# and found that they are useful when we need to perform several 
# tasks over whole data sets.

# Nesting a data frame is like converting every cell from a single
# observation into a list object.

# We print the nested data frame "covid_eu_nest" again
print(covid_eu_nest)
# A tibble: 55 x 2
# Groups: location [55]
#   location  data
# <chr>       <list>
# Albania     <tibble [1,320 x 66]>

# We can use `purrr::map(.x, .f)` to perform computations
# for every country at once.

# Moreover, we can create new columns to store the results,
# thus gathering all the information in the same object.

covid_eu_nest |> 
  unnest()

covid_eu_nest |> 
  mutate(
    max_total_cases = map_dbl(
      .x = data,
      .f = function(x) {
        x |> 
          pull(total_cases) |> 
          max(na.rm = TRUE)
      }
    ),
    min_total_cases = map_dbl(
      .x = data,
      .f = function(x) {
        x |> 
          pull(total_cases) |> 
          min(na.rm = TRUE)
      }
    )
  )

# Note that I used `dplyr::filter(!is.na(total_cases))`
# for the creation of "covid_eu_nest".


# Since each cell is a list object rather than an observation,
# we are by no means restricted to numeric elements.

# We could use the same strategy to create a column with
# plots, as discussed in the next section.


## 1.5 Graphics ----

# The grammar of graphics used in the `ggplot2` package
# is broad and far from simple at first glance.

# See more: https://ggplot2.tidyverse.org/

# The book by Wickham, Navarro, and Pedersen (2019) 
# is a useful reference: https://ggplot2-book.org/


# The process of making a graphic as a set of layers 
# arranged sequentially is what constitutes the
# "grammar of graphics".

# The first layer is the data you want to plot.

# We use the CPI data imported from the FRED API.

# The second layer is the `ggplot2::ggplot()` function.

# These two layers set the stage for what's to come.
cpi_tbl |> 
  ggplot()

graphics.off()

# The third layer we must provide is a `geom_*()` function,
# a geometry or *geom* that represents the data.

# For time series data, a line graph is created with
# `geom_line()`.

# We need to specify the values of the `x` and `y` axes
# inside of the `mapping = aes(x, y)` argument.

# We use the `dplyr::glimpse()` function to see 
# which values for `x` and `y` are appropriate
cpi_tbl |> 
  glimpse()
# Clearly, "date" fits the `x` and "value" the `y` axis.

# Note that the `ggplot2` package uses the plus (`+`)
# operator instead of the pipe `|>` operator to chain
# functions together.

# We add a meaningful title and labels for both x and y axes.
# We set them using the `labs()` layer.

# We also define shorter intervals for dates (x-axis) 
# and values (y-axis).


# We also want to highlight the upward trend of CPI as of 2021.
# Either the 12-month-accumulated CPI or the 3-months-accumulated
# seasonally-adjusted CPI would be common choices for this task,
# and we see examples on how to compute this later.

# For now we will use the average CPI from 2010 to 2019
# (pre-COVID period) as a measure of "normal" CPI.


# Since the parameter `x = date` appears in both 
# `geom_line()` layers, we can make the code more compact
# by moving this parameter to the `ggplot()` function.
# All parameters inside `ggplot()` are carried forward
# to every subsequent layer.

cpi_tbl |> 
  mutate(
    date = ymd(date),
    value = as.numeric(value),
    value_avg = mean(value[which(year(date) %in% 2010:2019)])
  ) |> 
  ggplot(mapping = aes(x = date)) +
  geom_line(mapping = aes(y = value), lwd = 2) +
  geom_line(mapping = aes(y = value_avg), color = "red", lwd = 2) +
  scale_x_date(date_breaks = "2 years", date_labels = "%Y") +
  scale_y_continuous(breaks = seq(from = -1, to = 1.5, by = 0.25), limits = c(-1, 1.5)) +
  labs(
    title = "US CPI showing an upward trend as of 2021",
    subtitle = "Red line is the 2010-2019 average",
    x = "Date",
    y = "US Monthly CPI (%)"
  ) +
  theme_bw()

ggsave(filename = "01_us-cpi-monthly-average.png", path = "figures/", height = 4, width = 8)
graphics.off()

# There is a subtle but important difference regarding the
# choice of using the `color` parameter inside the
# `geom_*()`'s aesthetics (`aes()`) or outside of it.

# If we use `color` as an attribute to hgihlight different
# groups from the data, then `color` goes inside `aes()`.

# In that case we pass a variable to `color`.

# If we simply specify a color for the *geom*, `color` goes 
# outside of `aes()`.


# We can create a new binary variable called "covid_period"
# that has the value "Yes" between the dates
# "2020-03-01" and "2022-12-01" and "No" otherwise.
cpi_tbl |> 
  mutate(
    date = ymd(date),
    value = as.numeric(value),
    covid_period = if_else(
      condition = between(date, left = ymd("2020-03-01"), right= ymd("2022-12-01")), 
      true = "Yes", false = "No")
  ) |> 
  ggplot(mapping = aes(x = date, y = value, color = covid_period)) +
  geom_line(lwd = 2) +
  labs(
    title = "US CPI showing an upward trend as of 2021",
    x = "Date",
    y = "US Monthly CPI (%)"
  ) +
  scale_x_date(date_breaks = "2 years", date_labels = "%Y") +
  scale_y_continuous(breaks = seq(from = -1, to = 1.5, by = 0.25), limits = c(-1, 1.5)) +
  theme_bw()

ggsave(filename = "01_us-cpi-monthly-covid-period.png", path = "figures/", height = 4, width = 8)
graphics.off()

# `ggplot()` automatically assigns colors for the attributes and adds
# a legend. We can customize the attribute's colors and legend position.

cpi_tbl |> 
  mutate(
    date = ymd(date),
    value = as.numeric(value),
    covid_period = if_else(
      condition = between(date, left = ymd("2020-03-01"), right= ymd("2022-12-01")), 
      true = "Yes", false = "No")
  ) |> 
  ggplot() +
  geom_line(mapping = aes(x = date, y = value, color = covid_period), lwd = 2) +
  labs(
    title = "US CPI showing an upward trend as of 2021",
    x = "Date",
    y = "US Monthly CPI (%)"
  ) +
  scale_x_date(date_breaks = "2 years", date_labels = "%Y") +
  scale_y_continuous(breaks = seq(from = -1, to = 1.5, by = 0.25), limits = c(-1, 1.5)) +
  scale_color_manual(values = c("darkgreen", "orange")) +
  theme(legend.position = "top") +
  theme_bw()

ggsave(filename = "01_us-cpi-monthly-covid-period.png", path = "figures/", height = 4, width = 8)
graphics.off()


# We will see how to build a scatter plot and explore some additional
# features from `ggplot()`.

# Scatter plots are used to highlight the relationship between two variables
# at a given moment in time.

# We use the Covid data for the date "2021-07-31".


# The `ggplot` package also works with tools such as
# HEX codes and the ColorBrewer.
# See more: https://colorbrewer2.org/

# ColorBrewer is a source for color schemes 
# designed to improve visual communication.

# We use `theme_light()` to make the plot cleaner.
# A theme is a set of pre-defined features for 
# a graph, and built-in themese include
# `theme_dark()`, `theme_minimal()` and `theme_bw()`.


# Note that this worls for 2022, but not for 2021.
covid_data |> 
  filter(date == "2022-07-31") |> 
  ggplot() +
  geom_point(
    mapping = aes(
      x = people_fully_vaccinated_per_hundred,
      y = new_deaths_smoothed_per_million,
      color = continent
      ), 
    size = 3
    ) +
  labs(
    title = "New deaths in late July were above 5 per million people in \n countries where vaccination falls below 30% of population"
  ) +
  scale_color_brewer(type = "qual", palette = 2) +
  theme_light()

ggsave(filename = "02_covid-deaths-vaccines.png", path = "figures/", height = 4, width = 8)
graphics.off()


# Instead of showing all continetns in a single plot we
# can separate them into multiple panels with
# `facet_wrap()`.

covid_data |> 
  filter(date == "2022-07-31", !is.na(continent)) |> 
  ggplot() +
  geom_point(
    mapping = aes(
      x = people_fully_vaccinated_per_hundred,
      y = new_deaths_smoothed_per_million
    ),
    size = 3
  ) +
  labs(
    title = "New deaths in late July were above 5 million people in \n countries where vaccination falls below 30% of population"
  ) +
  theme_light() +
  facet_wrap(facets = ~ continent)

ggsave(filename = "02_covid-deaths-vaccines-continents.png", path = "figures/", height = 4, width = 8)
graphics.off()

# Since we have a separate panel for each continent,
# it is no longer necessary to assign a different color
# for each of them.


## 1.6 Tables ----

# Graphs are the right tool to visualize historical data
# (time series) or relationships between variables (dot pltos).

# We often need to send reports with detailed data from the
# latest release of a certain variable.

# The `gt` package is the best choice when it comes to customizing
# tables.

# See more: https://gt.rstudio.com/

# The data frame "us_gdp" contains data on the contributions to
# percent change in real gross domestic product (annual rates)
# for the US economy released by the Bureau of Economic Analysis
# of the U.S. Department of Commerce (BEA).

# Note that the author provides this data as CSV file
# available on GitHub: https://github.com/leripio/R4ER/tree/main/data

# You can load the data with `readr::read_delim()`

us_gdp <- read_delim(file = "data/us_gdp.csv", delim = ";")
print(us_gdp)

us_gdp <- us_gdp |> 
  mutate(
    Quarter = as_date(Quarter, format = "%Y Q%q"),
    across(.cols = Goods:`Motor vehicle output`, .fns = ~ as.numeric(str_replace(string = .x, pattern = ",", replacement = ".")))
  )

print(us_gdp)

# The data frame is currently in wide format, and we pivot it to
# the long format.
library(gt)

us_gdp_gt <- us_gdp |> 
  pivot_longer(cols = -Quarter, names_to = "var", values_to = "value") |> 
  filter(Quarter >= "2021-01-01") |> 
  mutate(Quarter = paste0(year(Quarter), " Q", quarter(Quarter))) |> 
  pivot_wider(
    names_from = Quarter,
    values_from = value
  ) |> 
  gt(rowname_col = "var")

print(us_gdp_gt)

# This is a rough draft table.
# We can add some labs such as title, subtitle,
# and source note:
us_gdp_gt <- us_gdp_gt |> 
  tab_header(
    title = "Contributions to percent change in real gross domestic product",
    subtitle = "Annual rates"
  ) |> 
  tab_source_note(
    source_note = "Source: U.S. Bureau of Economic Analysis (BEA)"
  )

print(us_gdp_gt)


# The `tab_spanner` feature allows you to organize columns
# under a common label.

# We use one of its variations, `tab_spanner_delim`
# to organize the quarters under the respective year
# in order to make our table more compact.

us_gdp_gt <- us_gdp_gt |> 
  tab_spanner_delim(
    columns = -var,
    delim = " "
  )

print(us_gdp_gt)

gtsave(data = us_gdp_gt, filename = "01_us-gdp-product-contributions.png", path = "tables/")

# END